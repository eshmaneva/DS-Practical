# -*- coding: utf-8 -*-
"""
Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/176vFOTXiYvnVBSEzs6Yuii_G0HQ9hhMc


Classifying the summaries generated for the Amazon Food Products Reviews with the help of the T5 Model

---
(Summarization model is available under:
https://colab.research.google.com/drive/1kYVQqol5iIwEye1nm5cXZ27IkIMk1KB1?usp=sharing)

The task of **classification** can be defined as categorizing open-ended text into two or more predefined classes
based on some rules or similarities between these texts. It provides valuable insights
about unstructured text data as it divides them into classes.

"""

# Preinstalling the necessary libraries
# Certain versions are required to avoid compatibility issues 

!pip install --quiet numpy
!pip install --quiet tensorflow==2.7.0
!pip install --quiet transformers==4.7.0
!pip install --quiet sacremoses==0.0.45

# Importing necessary classes for classification and summarizaton
import tensorflow as tf
import tensorflow_datasets as tfds
from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification

import pandas as pd
import numpy as np

import nltk
import re

from six import viewitems

# Importing methods for splitting and shuffling data (as dataset contains no pre-trained data)
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit


# Mounting google drive to import the data from
from google.colab import drive
drive.mount('/content/drive')


# Reading and clearing the data
df=pd.read_csv("/data_input_direction", engine="python", error_bad_lines=False)


# Deleting all columns that are not usefull for training of the summarization model
df.drop(columns=['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator','HelpfulnessDenominator', 'Time'],axis=1,inplace=True)
print("Before",len(df))
df = df.dropna()
print("Data size:",len(df))
df.head()


# Classifying the data into two classes: positive and negative based on their star rating
df["Sentiment"] = df["Score"].apply(lambda score: "positive" if score >= 3 else "negative")
df['Sentiment'] = df['Sentiment'].map({'positive':1, 'negative':0})


# Creating a sunset of the data with only the required fields
df = df[["Text", "Sentiment"]]


# Shortening the data for testing purposes and comparing the shapes of the full and shortened set
# Remove in case the full training is possible
df=df.loc[1:100000]


# Dropping the na values
df.dropna()
df.head()


# Testing the labels
reviews = df['Text'].values.tolist()
labels = df['Sentiment'].tolist() #convert to category


# Shortening the dataset split into train and validation datasets 
# Used ratio 80/20
training_sentences, validation_sentences, training_labels, validation_labels = train_test_split(reviews, labels, test_size=.2)


#Creating stratifyed sample 
training_sentences, validation_sentences, training_labels, validation_labels = train_test_split(reviews, labels, test_size=0.33, random_state=42, stratify=labels)


# Preprocessing the data using DistilBert for punctuation splitting and wordpieces
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')


# Intiniating the tokenizer for the training data
tokenizer([training_sentences[0]], truncation=True, padding=True, max_length=128)


# Tokenizing the data
train_encodings = tokenizer(training_sentences,
                            truncation=True,
                            padding=True)
val_encodings = tokenizer(validation_sentences,
                            truncation=True,
                            padding=True)


# Slicing the dataset
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    training_labels
))

val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    validation_labels
))


# Loading the DistilBert model from transformers
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)


# Defining and fitting the model on the training data
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)
callbacks=tf.keras.callbacks.EarlyStopping(
    monitor='accuracy', 
    min_delta=0.0001,
    patience=3,
    mode='auto',
    verbose=2,
    baseline=None
)

model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])


# Training the model
model.fit(train_dataset.shuffle(100).batch(16),
          epochs=5,
          batch_size=16,
          validation_data=val_dataset.shuffle(100).batch(16),callbacks=callbacks)


#Saving the fitted model
model.save_pretrained("./sentiment")

loaded_model = TFDistilBertForSequenceClassification.from_pretrained("./sentiment")


# The Data is loaded from the Google.Drive,
# summaries presented in the data are generated with T5 Summarization Model
# (link to the model can be found in the description above)


# Importing the testing data from the T5 generated summaries
df1 = pd.read_csv("/data_input_direction", index_col=0, engine="python", error_bad_lines=False)
#df1=df1.loc[0:50] # use loc option in case when having not enough resources or the smaller sample is needed

selected_columns = df1[["Summary","Text","Generated_summary"]]
df = selected_columns.copy()
df = df.dropna()
df.head()

for i in range(0, len(df)):
  df['Sentiment_text'] = i 
  df['Sentiment_summary'] = i


# Classifying the text into sentiment classes
for i in range(0,len(df)):
  
  predict_input_text = tokenizer.encode(df['Text'][i],
                                 truncation=True,
                                 padding=True,
                                 return_tensors="tf")
  tf_output_text = loaded_model.predict(predict_input_text)[0]
  tf_prediction_text = tf.nn.softmax(tf_output_text, axis=1)
  labels = ['Negative','Positive']
  label_text = tf.argmax(tf_prediction_text, axis=1)
  label_text = label_text.numpy()
  df["Sentiment_text"][i] = (labels[label_text[0]])

  
#df = df.append(data, columns = "Sentiment")
#print(df['Text'], df['Sentiment_text'])


# Classifying the generated summaries into sentiment classes
for i in range(0, len(df)):
  
  predict_input_sum = tokenizer.encode(df['Generated_summary'][i],
                                 truncation=True,
                                 padding=True,
                                 return_tensors="tf")
  tf_output_sum = loaded_model.predict(predict_input_sum)[0]
  tf_prediction_sum = tf.nn.softmax(tf_output_sum, axis=1)
  #labels = ['Negative','Positive']
  label_sum = tf.argmax(tf_prediction_sum, axis=1)
  label_sum = label_sum.numpy()
  df["Sentiment_summary"][i] = (labels[label_sum[0]])

df.head()

"""
Calculation the error of the classification to analyse the performance of the classifier
The error is calculated as a percentage of the generated summaries,
          which mood differentiates from the mood of the user-written summary 
"""

# Calculating the error by checking the difference in classes for generated summary and text input
tag = 0
for i in range(0, len(df)):
  if (df['Sentiment_text'][i] != df['Sentiment_summary'][i]):
    tag = tag + 1

Error = tag/len(df)
print(Error)